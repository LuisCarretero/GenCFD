{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenCFD 2D Demo\n",
    "\n",
    "This demo showcases how the GenCFD diffusion model can be used as a surrogate model to solve the 2D wave equation, utilizing a sine basis representation. The wave equation is a second-order linear partial differential equation (PDE) that describes the evolution of waves or standing wave fields. Examples include sound waves, seismic waves, or other wave-like phenomena arising in fields such as acoustics, electromagnetism, or fluid dynamics.\n",
    "\n",
    "In this setting, the wave equation is a hyperbolic PDE described as a scalar function in both space and time. For fluid dynamics, the quantity of interest is often represented as\n",
    "\n",
    "$$\n",
    "u = u(x,y,t).\n",
    "$$\n",
    "\n",
    "Here u(x,y,t) represents physical quantities in scalar fields, such as pressure or displacement. The governing equation for these fields is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right) ,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- c: A fixed, non-negative real coefficient representing the wave propagation speed,\n",
    "- u: The scalar quantity of interest,\n",
    "- x,y: Spatial coordinates,\n",
    "- t: Time.\n",
    "\n",
    "This problem is fundamental in physics and engineering and serves as a building block for understanding more complex wave dynamics. In this demo, the GenCFD diffusion model acts as a surrogate to approximate the solution efficiently by leveraging the sine basis representation.\n",
    "\n",
    "For more information, visit the [Wave Equation](https://en.wikipedia.org/wiki/Wave_equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install GenCFD as a Library\n",
    "\n",
    "To use GenCFD as a library, you can install it directly from its GitHub repository. Before proceeding, ensure that you are working within a conda or virtual environment to manage dependencies effectively and avoid conflicts. Once the environment is set up, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this line out if you don't want to install GenCFD as a library\n",
    "!pip install git+https://github.com/camlab-ethz/GenCFD.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup relevant libraries\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import GenCFD\n",
    "from GenCFD import diffusion as dfn_lib\n",
    "from GenCFD import model, train, solvers, utils\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "array = np.ndarray\n",
    "\n",
    "DATA_STD = 0.5 # Fixed parameter but can also be learnable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "\n",
    "# Setting global seed for reproducibility\n",
    "torch.manual_seed(SEED)  # For CPU operations\n",
    "torch.cuda.manual_seed(SEED)  # For GPU operations\n",
    "torch.cuda.manual_seed_all(SEED)  # Ensure all GPUs (if multi-GPU) are set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Wave Equation\n",
    "\n",
    "In this demo, we solve the 2D wave equation using a sine basis representation. Below we detail the generation of the sine basis, the matrix describing the wave modes, and the analytical solution of the wave equation.\n",
    "\n",
    "1. Sine Basis Generation\n",
    "\n",
    "The sine basis is used to represent the solution as a sum of sinusoidal components. Each basis function corresponds to a specific mode (i,j), representing:\n",
    "\n",
    "$$\n",
    "sin(\\pi i x) sin(\\pi j y) .\n",
    "$$\n",
    "\n",
    "These functions are evaluated on a uniform grid of size s x s. The following function generates the sine basis of shape (K, K, s, s) where K is the number of modes in each spatial dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Sine basis B of the shape (K, K, s, s)\n",
    "# For each mode (i,j), B[i,j] represents the function \n",
    "# sin(np.pi*i*x)*np.sin(np.pi*j*y) evaluated at the s*s uniform grid\n",
    "def generate_sine_basis(K = 16, s = 128):\n",
    "    xx,yy = np.meshgrid(np.arange(s), np.arange(s), indexing=\"ij\")\n",
    "    xx = xx/s\n",
    "    yy = yy/s\n",
    "    sine_basis  = np.zeros((K,K,s,s))\n",
    "    for i in range(1,K+1):\n",
    "        for j in range(1,K+1):  \n",
    "            sine_basis[i-1,j-1] = np.sin(np.pi*i*xx)*np.sin(np.pi*j*yy)\n",
    "    return sine_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wave Mode Matrix\n",
    "\n",
    "To capture the frequency and decay properties of the wave equation, a square matrix is generated where each element (i,j) corresponds to:\n",
    "\n",
    "$$\n",
    "(i+1)^2 + (j+1)^2\n",
    "$$\n",
    "\n",
    "This matrix is essential for determining the mode-dependent contributions to the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_matrix(K=16):\n",
    "    M = np.zeros((K, K))\n",
    "    for i in range(1, K + 1):\n",
    "        for j in range(1, K + 1):\n",
    "            M[i - 1, j - 1] = i**2 + j**2\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analytical Solution of the 2D Wave Equation\n",
    "\n",
    "The analytical solution is computed using the sine basis and wave mode matrix. The scalar field u(x,y,t) is represented as a summation of sinusoidal components, modulated by coefficients, decay terms, and time-dependent oscillations:\n",
    "\n",
    "- f(x,y): Initial condition of the wave equation.\n",
    "- u(x,y,t): Solution at a given time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution of 2d Wave equation\n",
    "def generate_solution_wave(coeff, time, sine_basis, square_matrix, K = 16, decay = 0.8, c = 0.1):\n",
    "    multiplier_f = coeff * np.power(square_matrix, decay)\n",
    "    multiplier_f = multiplier_f.reshape(K, K , 1, 1)\n",
    "    f = np.pi*np.sum(multiplier_f * sine_basis, axis = (0,1))\n",
    "\n",
    "    square_matrix_time = np.cos(c * np.pi * time * np.sqrt(square_matrix))\n",
    "    multiplier_u = coeff * np.power(square_matrix, decay) * square_matrix_time\n",
    "    multiplier_u = multiplier_u.reshape(K, K , 1, 1)\n",
    "    u = np.pi*np.sum(multiplier_u * sine_basis, axis = (0,1))\n",
    "\n",
    "    return f, u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-Macro Perturbations\n",
    "\n",
    "GenCFD has demonstrated the ability to capture not only statistical patterns but also make accurate predictions for out-of-distribution scenarios. To evaluate this capability, GenCFD is tested on a dataset generated with small perturbations in the initial conditions - perturbations it has never encountered during training.\n",
    "\n",
    "these minor variations in the initial conditions can lead to significant changes in the target solution, a characteristic commonly observed in turbulent flows and chaotic systems. Consequently, this dataset is exclusively used for inference and validation, highlighting GenCFD's robustness in handling unpredictable and dynamic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of micro-macro perturbations\n",
    "def generate_perturbation_wave(coeff, time, sine_basis, square_matrix, K = 16, decay = 0.8, c =0.1, perturbation = 0.25):\n",
    "    p_matrix = np.random.uniform(-perturbation, perturbation, (K, K))\n",
    "    return generate_solution_wave(coeff+p_matrix , time, sine_basis, square_matrix, K = K, decay = decay, c = c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation for Training and Evaluation\n",
    "\n",
    "The generate_training_data function creates a dataset of input-output pairs for training the model, while the generate_micro_macro_data function generates datasets with micro and macro perturbations for evaluating the model's robustness to chaotic and turbulent scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(c = 0.1, time = 5.0, N_data = 1024, K = 24, decay = -0.8, s = 128):\n",
    "    sine_basis = generate_sine_basis(K, s)\n",
    "    square_matrix = generate_square_matrix(K)\n",
    "\n",
    "    inputs = np.zeros((N_data, s, s))\n",
    "    targets = np.zeros((N_data, s, s))\n",
    "    \n",
    "    for n in range(N_data):\n",
    "        coeff = np.random.uniform(-1,1, (K, K))\n",
    "        inp, out = generate_solution_wave(coeff, time, sine_basis, square_matrix, K = K, decay = decay, c = c)\n",
    "        inputs[n] = inp\n",
    "        targets[n] = out\n",
    "\n",
    "        if n%20 == 0:\n",
    "            print(f\"Done {n+1} out of {N_data}\")\n",
    "        \n",
    "    print(\" \")\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "def generate_micro_macro_data(coeff_macro, perturbation = 0.25, c = 0.1, time = 5.0, N_data = 1024, K = 24, decay = -0.8, s = 128):\n",
    "    sine_basis = generate_sine_basis(K, s)\n",
    "    square_matrix = generate_square_matrix(K)\n",
    "\n",
    "    inputs = np.zeros((N_data, s, s))\n",
    "    targets = np.zeros((N_data, s, s))\n",
    "    \n",
    "    for n in range(N_data):\n",
    "        inp, out = generate_perturbation_wave(coeff_macro, time, sine_basis, square_matrix, K = K, decay = decay, c = c, perturbation = perturbation)\n",
    "        inputs[n] = inp\n",
    "        targets[n] = out\n",
    "\n",
    "        if n%20 == 0:\n",
    "            print(f\"Done {n+1} micro out of {N_data}\")\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of training and evaluation samples. Defining key parameters for the wave equation simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 24\n",
    "# It's possible to choose a smaller resolution s, e.g. 64\n",
    "s = 128\n",
    "decay = -0.8\n",
    "c = 0.1\n",
    "time = 5.0\n",
    "N_data = 128\n",
    "\n",
    "inp_train, out_train = generate_training_data(c = c, time = time, N_data = N_data, K = K, decay =decay, s = s)\n",
    "\n",
    "N_data_macro = 128\n",
    "perturbation = 0.25\n",
    "coeff_macro = np.random.uniform(-1,1, (K, K))\n",
    "\n",
    "inp_micro, out_micro = generate_micro_macro_data(coeff_macro, perturbation = perturbation, c = c, time = time, N_data = N_data_macro, K = K, decay = decay, s = s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuzalization\n",
    "\n",
    "The Initial Condition represents the input to the model, while the Solution repersents the target output that the model aims to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(inp_train[0,...])\n",
    "axes[0].set_title(\"Initial Condition\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(out_train[0,...])\n",
    "axes[1].set_title(\"Solution\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaseDataset class serves as a foundation for managing both training and evaluation datasets. It standardizes data input/output for consistent model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset:\n",
    "    \"\"\"\n",
    "    Base class for Inference and Training. \n",
    "\n",
    "    Args:\n",
    "        input_data: Training Input Dataset, typically includes the initial conditions\n",
    "        output_data: Ground Truth (GT), Output Dataset, is the solution to the given problem\n",
    "        mode: Decision whether the given data is used for inference or training thus only 2 options ['training', 'evaluation']\n",
    "        input_channel: Number of input channels (from train_data)\n",
    "        output_channel: Number of channels that should be predicted by the Diffusion Model\n",
    "        spatial_resolution: Resolution of the input and output dataset\n",
    "        input_shape: Shape of the input data tensor. Depends on the conditioning you use\n",
    "        output_shape: This should be equal to (output_channel, spatial_resolution)\n",
    "        ndim: Dimensionality of the dataset. Here we have a 2D dataset\n",
    "        training_samples: Number of samples available in the dataset\n",
    "        mean_training_input: Array with relevant mean values for each channel of the input dataset\n",
    "        std_training_input: Array with relevant std values for each channel of the input dataset\n",
    "        mean_training_output: Array with relevant mean values for each channel of the output dataset\n",
    "        std_training_output: Array with relevant std values for each channel of the output dataset\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_data: Dataset,\n",
    "            output_data: Dataset,\n",
    "            mode: str,\n",
    "            input_channel: int,\n",
    "            output_channel: int,\n",
    "            spatial_resolution: Tuple,\n",
    "            input_shape: Tuple,\n",
    "            output_shape: Tuple,\n",
    "            ndim: int = 2,\n",
    "            training_samples: int = None,\n",
    "            mean_training_input: array = None,\n",
    "            std_training_input: array = None,\n",
    "            mean_training_output: array = None,\n",
    "            std_training_output: array = None\n",
    "        ) -> None:\n",
    "\n",
    "        assert mode in ['training', 'evaluation'], f'mode can either be training or evaluation not {mode}'\n",
    "\n",
    "        self.initial_cond = input_data\n",
    "        self.target_cond = output_data\n",
    "        self.mode = mode\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.spatial_resolution = spatial_resolution\n",
    "        self.input_shape = input_shape,\n",
    "        self.output_shape = output_shape,\n",
    "        self.ndim = ndim,\n",
    "        self.training_samples = training_samples\n",
    "        self.mean_training_input = mean_training_input\n",
    "        self.std_training_input = std_training_input\n",
    "        self.mean_training_output = mean_training_output\n",
    "        self.std_training_output = std_training_output\n",
    "\n",
    "\n",
    "    def normalize(self, u_: Union[array, Tensor], which: str) -> Union[array, Tensor]:\n",
    "        \"\"\"Standardization of data can be done for torch tensors or numpy arrays\n",
    "        \n",
    "        args:\n",
    "            u_:     Container which should be standardized by subtracting the mean and dividing through the std\n",
    "            which:   decision whether to use the input or output normalization parameters (mean and std) \n",
    "        \"\"\"\n",
    "\n",
    "        assert which in ['input', 'output']\n",
    "\n",
    "        if self.mean_training_input is not None:\n",
    "            mean_training_input = self.mean_training_input if which == 'input' else self.mean_training_output\n",
    "            std_training_input = self.std_training_input if which == 'input' else self.std_training_output\n",
    "            if isinstance(u_, Tensor):\n",
    "                mean_training_input = torch.as_tensor(\n",
    "                    mean_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "                std_training_input = torch.as_tensor(\n",
    "                    std_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "            return (u_ - mean_training_input) / (std_training_input + 1e-12)\n",
    "        else:\n",
    "            return u_\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        \n",
    "        initial_cond = self.initial_cond[index, ...].reshape((*self.spatial_resolution, 1))\n",
    "        target_cond = self.target_cond[index, ...].reshape((*self.spatial_resolution, 1))\n",
    "\n",
    "        if self.mode == 'training':\n",
    "            initial_cond = self.normalize(initial_cond, 'input')\n",
    "            target_cond = self.normalize(target_cond, 'output')\n",
    "        \n",
    "        # The data should be in the form (channel, y, x)\n",
    "        initial_condition = (\n",
    "            torch.from_numpy(initial_cond)\n",
    "            .type(torch.float32)\n",
    "            .permute(2, 1, 0)\n",
    "        )\n",
    "\n",
    "        target_condition = (\n",
    "            torch.from_numpy(target_cond)\n",
    "            .type(torch.float32)\n",
    "            .permute(2, 1, 0)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'initial_cond': initial_condition,\n",
    "            'target_cond': target_condition\n",
    "        }\n",
    "\n",
    "\n",
    "mean_training_input = inp_train.mean()\n",
    "std_training_input = inp_train.std()\n",
    "mean_training_output = out_train.mean()\n",
    "std_training_output = out_train.std()\n",
    "\n",
    "batch_size = 5\n",
    "in_channels = 1 # number of channels of the initial condition\n",
    "out_channels = 1 # number of channels to predict\n",
    "spatial_resolution = (s, s) # (128, 128)\n",
    "input_shape = (2, s, s) # due to conditioning of the diff model on intitial cond\n",
    "output_shape = (out_channels, s, s) \n",
    "\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    input_data=inp_train,\n",
    "    output_data=out_train,\n",
    "    mode='training',\n",
    "    input_channel=in_channels,\n",
    "    output_channel=out_channels,\n",
    "    spatial_resolution=spatial_resolution, # (128, 128)\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    ndim=2,\n",
    "    training_samples=N_data, # 128 samples\n",
    "    mean_training_input=mean_training_input,\n",
    "    std_training_input=std_training_input,\n",
    "    mean_training_output=mean_training_output,\n",
    "    std_training_output=std_training_output\n",
    ")\n",
    "\n",
    "evaluation_dataset = BaseDataset(\n",
    "    input_data=inp_micro,\n",
    "    output_data=out_micro,\n",
    "    mode='evaluation',\n",
    "    input_channel=in_channels,\n",
    "    output_channel=out_channels,\n",
    "    spatial_resolution=spatial_resolution, \n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    ndim=2,\n",
    "    training_samples=N_data, \n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "evaluation_dataloader = DataLoader(\n",
    "    dataset=evaluation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "Section describes the creation and configuration of the denoising model, which consists of a UNet-based Preconditioned Denoiser wrapped within a diffusion framework for generating samples. To enhance performance, the model can be compiled using torch.compile. This provides faster training and inference, particularly on GPUs.\n",
    "\n",
    "Note: Some GPUs may produce warnings during compilation. These warnings can safely be ignored. For the following GPUs no Warnings were observed:\n",
    "- NVIDIA GeForce RTX 2080 Ti\n",
    "- NVIDIA GeForce RTX 3090\n",
    "- NVIDIA GeForce RTX 4090\n",
    "- NVIDIA Tesla A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = model.PreconditionedDenoiser(\n",
    "    in_channels=in_channels + out_channels, # Conditioning thus stacked input and output\n",
    "    out_channels=out_channels,\n",
    "    spatial_resolution=spatial_resolution,\n",
    "    time_cond=False,\n",
    "    num_channels=(64, 128),\n",
    "    downsample_ratio=(2, 2),\n",
    "    num_blocks=4,\n",
    "    noise_embed_dim=128,\n",
    "    output_proj_channels=128,\n",
    "    input_proj_channels=128,\n",
    "    padding_method='circular',\n",
    "    dropout_rate=0.0,\n",
    "    use_attention=True,\n",
    "    use_position_encoding=True,\n",
    "    num_heads=8,\n",
    "    normalize_qk=False,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    "    sigma_data=DATA_STD\n",
    ")\n",
    "\n",
    "# Optional: compile the model for fast training and inference\n",
    "is_compiled = False\n",
    "if torch.cuda.is_available() and is_compiled:\n",
    "    denoiser = torch.compile(denoiser)\n",
    "    # For some GPUs there will appear Warnings, if that's the case \n",
    "    # these can be ignored.\n",
    "    warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(device=device),\n",
    "    data_std=DATA_STD,\n",
    ")\n",
    "\n",
    "denoising_model = dfn_lib.DenoisingModel(\n",
    "    spatial_resolution=spatial_resolution,\n",
    "    time_cond=False,\n",
    "    denoiser=denoiser,\n",
    "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
    "        diffusion_scheme, \n",
    "        clip_min=1e-4, \n",
    "        uniform_grid=True, \n",
    "        device=device\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(\n",
    "        data_std=DATA_STD, \n",
    "        device=device\n",
    "    ),\n",
    "    consistent_weight=0.0,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Print number of Parameters:\n",
    "model_params = sum(\n",
    "    p.numel() for p in denoising_model.denoiser.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"Total number of model parameters: {model_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DenoisingTrainer handles the training process for the denoising model. It includes features like Exponential Moving Average (EMA), which is crucial for stabilizing training and should always be enabled for consistent and reliable inference. Additionally, mixed precision training is utilized to optimize memory usage and computational efficiency by leveraging float16 or bfloat16 precision, making it ideal for modern GPUs and large-scale models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = train.DenoisingTrainer(\n",
    "    model=denoising_model,\n",
    "    optimizer=optim.AdamW(\n",
    "        denoising_model.denoiser.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    device=device,\n",
    "    ema_decay=0.999,\n",
    "    store_ema=True,\n",
    "    track_memory=False,\n",
    "    use_mixed_precision=True,\n",
    "    is_compiled=is_compiled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command launches the training, where metric_aggregation_steps specifies the interval, in terms of iteration steps, after which metrics like loss or the standard deviation of the loss should be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = 1_000\n",
    "\n",
    "train.run_training(\n",
    "    train_dataloader=train_dataloader,\n",
    "    trainer=trainer,\n",
    "    workdir=str(os.getcwd()),\n",
    "    # DDP configs (not used here thuse set to 1)\n",
    "    world_size=1,\n",
    "    local_rank=-1,\n",
    "    # Training configs\n",
    "    total_train_steps=num_train_steps,\n",
    "    # Since the dataset is small, metric aggregation after every epoch\n",
    "    metric_aggregation_steps=N_data // batch_size, \n",
    "    # Callbacks\n",
    "    callbacks=(\n",
    "        # Save model at every checkpoint or at the end of the training\n",
    "        train.callbacks.TrainStateCheckpoint(\n",
    "            base_dir=str(os.getcwd()),\n",
    "            save_every_n_step=num_train_steps # only save one model at the end\n",
    "        ),\n",
    "        # Display training progress in a tqdm bar\n",
    "        train.callbacks.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=[\"loss\",]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The model's latest checkpoint is restored from the specified directory. Relevant is to load the EMA model parameters for inference on the macro-micro perturbed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(str(os.getcwd()),'checkpoints')\n",
    "latest_model_path = utils.get_latest_checkpoint(model_path)\n",
    "\n",
    "trained_state = train.TrainState.restore_from_checkpoint(\n",
    "    latest_model_path,\n",
    "    model=denoising_model.denoiser,\n",
    "    optimizer=trainer.optimizer,\n",
    "    is_compiled=trainer.is_compiled,\n",
    "    is_parallelized=False,\n",
    "    use_ema=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Stochastic Differential Equation\n",
    "\n",
    "These functions are necessary to solve the stochastic differential equation (SDE), as the diffusion model predicts the score function. To solve this equation the Euler-Maruyama method is employed for numerical integration while the EDM noise decay function defines the time span and noise schedule. The sampler then uses these components to generate the resulting sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "denoise_fn = trainer.inference_fn_from_state_dict(\n",
    "    trained_state,\n",
    "    denoiser=denoising_model.denoiser,\n",
    "    lead_time=False,\n",
    ")\n",
    "\n",
    "integrator = solvers.EulerMaruyama(\n",
    "    time_axis_pos=0,\n",
    "    terminal_only=True\n",
    ")\n",
    "\n",
    "tspan = dfn_lib.edm_noise_decay(\n",
    "    scheme=diffusion_scheme,\n",
    "    rho=7,\n",
    "    # Minimum of 30 steps required\n",
    "    num_steps=128,\n",
    "    end_sigma=1e-3,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=output_shape,\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=denoise_fn,\n",
    "    tspan=tspan,\n",
    "    integrator=integrator,\n",
    "    guidance_transforms= (),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize resulting sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(evaluation_dataloader))  # uniform random distribution\n",
    "batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "u0 = batch[\"initial_cond\"]\n",
    "u = batch[\"target_cond\"]\n",
    "\n",
    "# No lead_time required since model wasn't conditioned on time\n",
    "lead_time = [None] * batch_size\n",
    "\n",
    "gen_samples = sampler.generate(\n",
    "    num_samples=batch_size,\n",
    "    y=u0,\n",
    "    lead_time=lead_time,\n",
    ").detach()\n",
    "\n",
    "utils.plot_2d_sample(\n",
    "    gen_sample=gen_samples[0], \n",
    "    gt_sample=u[0], \n",
    "    axis=0, \n",
    "    save=False, \n",
    "    save_dir=str(os.getcwd())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
