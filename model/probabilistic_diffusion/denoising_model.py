# Copyright 2024 The swirl_dynamics Authors and adaptations made
# by the CAM Lab at ETH Zurich.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Training a denoising model for diffusion-based generation."""

import dataclasses
from typing import Any, Protocol, Optional, Mapping

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchmetrics import MetricCollection
import numpy as np

import diffusion as dfn_lib
from model.base_model import base_model

from GPUtil.GPUtil import getGPUs

Tensor = torch.Tensor
Metrics = dict # Placeholder for metrics that are implemented!

class DenoisingTorchModule(Protocol):
  """Expected interface of the flax module compatible with `DenoisingModel`.
  For the PyTorch based version we don't need to worry about that!

  NOTE: This protocol is for reference only and not statically checked.
  """

  def forward(
      self, x: Tensor, y: Tensor, time: Tensor, sigma: Tensor, is_training: bool
      ) -> Tensor:
    ...


@dataclasses.dataclass(frozen=True, kw_only=True)
class DenoisingModel(base_model.BaseModel):
  """Trains a model to remove Gaussian noise from samples.

  Attributes:
    input_shape: Shape of a single sample (excluding any batch dimensions).
    denoiser: The flax module for denoising. Its `__call__` method should adhere
      to the `DenoisingFlaxModule` interface.
    noise_sampling: Callable for generating noise levels during training.
    noise_weighting: Callable for calculating loss weights based on noise
      levels.
    cond_shape: Dictionary mapping conditional input names to their shapes (as
      tuples). This allows for nested structures if needed. If set to `None`,
      the model is assumed to be unconditional.
    num_eval_cases_per_lvl: Number of evaluation samples created per noise
      level. These are generated by adding noise to random members of the
      evaluation batch.
    min_eval_noise_lvl: Minimum noise level used during evaluation.
    max_eval_noise_lvl: Maximum noise level used during evaluation.
    num_eval_noise_levels: Number of noise levels for evaluation (log-uniformly
      spaced between the minimum and maximum).
  """

  input_shape: tuple[int, ...]
  denoiser: nn.Module
  noise_sampling: dfn_lib.NoiseLevelSampling
  noise_weighting: dfn_lib.NoiseLossWeighting
  rng_diff: torch.Generator
  rng: torch.Generator
  seed: int = 0
  num_eval_noise_levels: int = 5
  num_eval_cases_per_lvl: int = 1
  min_eval_noise_lvl: float = 1e-3
  max_eval_noise_lvl: float = 50.0
  
  input_channel: int = 1
  which_tspan: str = 'exponential_noise_decay'
  consistent_weight: float = 0
  compute_crps: bool = False
  device: Any | None = None
  dtype: torch.dtype = torch.float32


  def initialize(self, batch_size: int):
    """Method necessary for a dummy initialization!"""
    x_sample = torch.ones((batch_size,) + self.input_shape, dtype=self.dtype, device=self.device)
    # x = x_sample[:,self.input_channel:, ...]
    # y = x_sample[:, :self.input_channel, ...]
    # TODO: Include the TIME once this model is built!
    # return self.denoiser(
    #     x=x, y=y, time=torch.ones((1,)), sigma=torch.ones((1,)), is_training=False
    # )
    return self.denoiser(
        x=x_sample, sigma=torch.ones((batch_size,), dtype=self.dtype, device=self.device), is_training=False
    )

  def loss_fn(
      self,
      # params: dict,
      batch: dict,
      mutables: Optional[dict] = None
  ):
    """Computes the denoising loss on a training batch.

    Args:
      params: The parameters of the denoising model to differentiate against.
      batch: A batch of training data expected to contain an `x` field with a
        shape of `(batch, *spatial_dims, channels)`, representing the unnoised
        samples. Optionally, it may also contain a `cond` field, which is a
        dictionary of conditional inputs.
      rng: Random key for training use.
      mutables: The mutable (non-diffenretiated) parameters of the denoising
        model (e.g. batch stats); *currently assumed empty*.

    Returns:
      The loss value and a tuple of training metric and mutables.
    """

    # data = batch["data"]
    data = batch

    batch_size = len(data)

    x = data[:,self.input_channel:, ...]
    y = data[:, :self.input_channel, ...]

    # x_squared = torch.square(x)
    x_squared = torch.square(data)

    sigma = self.noise_sampling(rng=self.rng_diff, shape=(batch_size,)).to(device=self.device)
    weights = self.noise_weighting(sigma)
    if weights.ndim != x.ndim:
      weights = weights.view(-1, *([1] * (x.ndim - 1)))

    noise = torch.randn(
      data.shape, dtype=self.dtype, device=self.device, generator=self.rng
      )

    # noise = x + noise * sigma
    if sigma.ndim != x.ndim:
      noised = data + noise * sigma.view(-1, *([1] * (x.ndim - 1))) 
    else:
      noised = data + noise * sigma
    
    # denoised = self.denoiser.forward(x=noised, y=y, time=time, sigma=sigma, is_training=True)
    denoised = self.denoiser.forward(x=noised, sigma=sigma, is_training=True)
    denoised_square = torch.square(denoised)

    # rel_norm = torch.mean(torch.square(x) / torch.mean(torch.square(x_squared)))
    # loss = torch.mean(weights * torch.square(denoised - x))
    rel_norm = torch.mean(torch.square(data) / torch.mean(torch.square(x_squared)))
    loss = torch.mean(weights * torch.square(denoised - data))
    loss += self.consistent_weight * rel_norm * \
            torch.mean(weights * torch.square(denoised_square - x_squared))
    
    metric = {
      "loss": loss.item(),
      "mem": 0. # TODO: Placeholder for memory metric!
    }

    # print(f"metric_loss: {loss.item()}")

    return loss, (metric, mutables)
  

  def eval_fn(
      self,
      # variables: dict,
      batch: dict
  ):
    """Compute denoising metrics on an eval batch.

    Randomly selects members of the batch and noise them to a number of fixed
    levels. Each level is aggregated in terms of the average L2 error.

    Args:
      variables: Variables for the denoising module.
      batch: A batch of evaluation data expected to contain an `x` field with a
        shape of `(batch, *spatial_dims, channels)`, representing the unnoised
        samples. Optionally, it may also contain a `cond` field, which is a
        dictionary of conditional inputs.
      rng: Random key for evaluation use.

    Returns:
      A dictionary of denoising-based evaluation metrics.
    """
    # time = batch["lead_time"]
    # data = batch["data"]
    data = batch
    inputs = data[
      torch.randint(0, data.shape[0], (self.num_eval_noise_levels, self.num_eval_cases_per_lvl), 
                    generator=self.rng, device=self.device)
      ]

    sigma = torch.exp(
      torch.linspace(
        np.log(self.min_eval_noise_lvl),
        np.log(self.max_eval_noise_lvl),
        self.num_eval_noise_levels, 
        dtype=self.dtype,
        device=self.device
      )
    )

    # noise = torch.randn_like(x)
    # noised = x + noise * sigma.unsqueeze(-1)
    # denoise_fn = self.inference_fn(variables, self.denoiser)
    # denoised = torch.stack([denoise_fn(noised[i], y[i], inputs_time[i], sigma[i]) for i in range(len(sigma))])

    # ema_losses = torch.sqrt(torch.mean(torch.square(denoised - x), dim=-1) / torch.mean(torch.square(x), dim=-1))

    noise = torch.randn(
      inputs.shape, device=self.device, dtype=self.dtype, generator=self.rng
      )
    
    if sigma.ndim != inputs.ndim:
      noised = inputs + noise * sigma.view(-1, *([1] * (inputs.ndim - 1))) 
    else:
      noised = inputs + noise * sigma

    denoised = torch.stack(
      [self.inference_fn(self.denoiser, noised[i], sigma[i]) for i in range(self.num_eval_noise_levels)]
      )

    ema_losses = torch.mean(torch.square(denoised - inputs), dim=[i for i in range(1, inputs.ndim)])

    eval_losses = {f"eval_denoise_lvl{i}": loss.item() for i, loss in enumerate(ema_losses)}
    return eval_losses


  @staticmethod
  def inference_fn(denoiser: nn.Module) -> Tensor:
    """Returns the inference denoising function."""

    # def _denoise(x: Tensor, y: Tensor, time: float | Tensor, sigma: float | Tensor) -> Tensor:
    #   if not torch.is_tensor(sigma):
    #     sigma = sigma * torch.ones((x.shape[0],))
    #   return denoiser.forward(x=x, y=y, sigma=sigma, is_training=False)

    def _denoise(
        x: Tensor, sigma: float | Tensor, cond: Mapping[str, Tensor] | None = None
      ) -> Tensor:
    
      if not torch.is_tensor(sigma):
        sigma = sigma * torch.ones((x.shape[0],))
  
      return denoiser.forward(x=x, sigma=sigma, is_training=False)
  
    return _denoise